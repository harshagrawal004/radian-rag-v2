# RAG Performance Testing

This directory contains scripts for testing and evaluating RAG performance.

## Setup

1. **Run the migration** to create the `rag_test_as` table:
   ```sql
   -- Copy contents of backend/migrations/004_create_rag_test_as_table.sql
   -- into Supabase SQL Editor and run it
   ```

2. **Ensure test data exists** in `rag_test_results` table with columns:
   - `id` (UUID or unique identifier)
   - `question` (text)
   - `answer_must_include` (text - requirements that must be in the answer)
   - `file_name` (optional)
   - `page_number` (optional)

## Running Tests

### Basic Usage

```bash
cd backend
python -m scripts.test_rag_performance --patient-id Sanjeev
```

### With Custom Run ID

```bash
python -m scripts.test_rag_performance --patient-id Sanjeev --run-id my-test-run-001
```

### Without LLM Judge (Faster)

```bash
python -m scripts.test_rag_performance --patient-id Sanjeev --no-llm-judge
```

## Scoring System

The test script uses a two-part scoring system:

1. **Must-Include Score (0-1)**: Checks if required phrases from `answer_must_include` are present in the RAG answer
2. **LLM Judge Score (0-5)**: Uses GPT-4o-mini to semantically evaluate answer quality
3. **Final Score (0-5)**: Combines both: `0.4 * (must_score * 5) + 0.6 * judge_score`
4. **Pass/Fail**: 
   - Must-include score >= 0.7 AND
   - Final score >= 3.5

## Output

Results are stored in the `rag_test_as` table with:
- `rag_answer`: The answer generated by RAG
- `score`: Final score (0-5)
- `must_include_score`: Must-include check score (0-1)
- `judge_score`: LLM judge score (0-5, nullable)
- `pass_fail`: Boolean pass/fail result
- `missing_must_include`: List of missing requirements
- `judge_rationale`: LLM's explanation of the score

## Viewing Results

Query the results:

```sql
SELECT 
    run_id,
    question,
    score,
    pass_fail,
    must_include_score,
    judge_score,
    missing_must_include
FROM rag_test_as
WHERE run_id = 'your-run-id'
ORDER BY score DESC;
```

## Configuration

Edit the script to adjust:
- `MUST_INCLUDE_THRESHOLD = 0.7` - Minimum must-include score
- `PASS_SCORE_THRESHOLD = 3.5` - Minimum final score to pass
- `USE_LLM_JUDGE = True` - Enable/disable LLM judge
- `K = 15` - Number of chunks to retrieve
- `JUDGE_MODEL = "gpt-4o-mini"` - Model for LLM judge

